{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b710caa-1a38-4e27-b832-37262fff4808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Dataset shape: (591892, 33)\n",
      "Columns: ['subject_id', 'stay_id', 'intime', 'outtime', 'gender', 'race', 'chiefcomplaint', 'anchor_age', 'anchor_year', 'anchor_year_group', 'dod', 'dead_in_days', 'died_within_30_days', 'race_standard', 'age_group', 'unique_visit_id', 'terms', 'terms_new', 'indiv_symptom', 'counter', 'unique_ids_exploded', 'expanded_symptoms', 'expanded_symptoms_new', 'counter_new', 'unique_ids_exploded_new', 'snomed', 'is_male', 'ed_age', 'symptom', 'mu', 'sigma', 'mortality_prob', 'mortality_percent']\n",
      "\n",
      "\n",
      "=== DATA PREPARATION FOR THRESHOLD ANALYSIS ===\n",
      "Filtering to rows with valid predictions (non-missing mortality_prob and label)...\n",
      "Before filtering: 591,892 rows\n",
      "After filtering: 232,895 rows with predictions\n"
     ]
    }
   ],
   "source": [
    "# Your existing imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import roc_auc_score  # Add this import\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: LOAD YOUR DATA\n",
    "# =============================================================================\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('dataset/ed/finals/16_finalwithsigmoid.csv')\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: DATA CLEANING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n=== DATA PREPARATION FOR THRESHOLD ANALYSIS ===\")\n",
    "print(\"Filtering to rows with valid predictions (non-missing mortality_prob and label)...\")\n",
    "print(f\"Before filtering: {len(df):,} rows\")\n",
    "\n",
    "# Keep only rows with predictions & labels\n",
    "df_clean = df.dropna(subset=['mortality_prob', 'died_within_30_days']).copy()\n",
    "\n",
    "# Coerce types & clip probabilities to valid range [0,1]\n",
    "df_clean['mortality_prob'] = pd.to_numeric(df_clean['mortality_prob'], errors='coerce').clip(0, 1)\n",
    "df_clean['died_within_30_days'] = pd.to_numeric(df_clean['died_within_30_days'], errors='coerce').astype(int)\n",
    "\n",
    "# Drop any rows that may have turned NaN after coercion\n",
    "df_clean = df_clean.dropna(subset=['mortality_prob', 'died_within_30_days'])\n",
    "\n",
    "# Also fix dtype for symptom column if it exists\n",
    "if 'symptom' in df_clean.columns:\n",
    "    df_clean['symptom'] = pd.to_numeric(df_clean['symptom'], errors='coerce').astype('Int64')\n",
    "\n",
    "print(f\"After filtering: {len(df_clean):,} rows with predictions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515f92a1-8b45-4a82-b8aa-ebc170614087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# CALIBRATION CHECKER (MEASUREMENT ONLY, WITH \"WHY IT'S GOOD\" NOTES)\n",
    "# ============================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple, Optional\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# ---------- Utility ----------\n",
    "def _safe_logit(p: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert probabilities p in (0,1) to logits log(p/(1-p)).\n",
    "    Clipping avoids infinities when p is 0 or 1.\n",
    "    Why it’s good: using logits makes linear calibration modeling stable and numerically well-behaved.\n",
    "    \"\"\"\n",
    "    p = np.clip(np.asarray(p, dtype=float), eps, 1 - eps)\n",
    "    return np.log(p / (1 - p))\n",
    "\n",
    "# ---------- Core calibration metrics ----------\n",
    "def calibration_slope_intercept(y: np.ndarray, p: np.ndarray) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Estimate calibration-in-the-large (intercept) and calibration slope (slope) by fitting:\n",
    "      logit(E[Y]) = a + b * logit(p).\n",
    "    Ideal: intercept a ≈ 0, slope b ≈ 1. Returns (a, b); (nan, nan) if y has one class.\n",
    "    Why it’s good: data-efficient summary used widely in clinical prediction; quickly tells if risks are globally shifted (a) or over/under-confident (b).\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, dtype=int)\n",
    "    if len(np.unique(y)) < 2:\n",
    "        return float(\"nan\"), float(\"nan\")\n",
    "    z = _safe_logit(p).reshape(-1, 1)\n",
    "    lr = LogisticRegression(solver=\"lbfgs\", max_iter=1000)\n",
    "    lr.fit(z, y)\n",
    "    a = float(lr.intercept_[0])\n",
    "    b = float(lr.coef_[0][0])\n",
    "    return a, b\n",
    "\n",
    "def ece_mce(y: np.ndarray, p: np.ndarray, n_bins: int = 15) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Expected Calibration Error (ECE) and Maximum Calibration Error (MCE) via uniform-probability bins.\n",
    "    - ECE: weighted average |observed - predicted| per bin.\n",
    "    - MCE: largest absolute gap across bins. Smaller is better.\n",
    "    Why it’s good: ECE/MCE give intuitive, threshold-free calibration gaps that are easy to explain and visualize.\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, dtype=int)\n",
    "    p = np.clip(np.asarray(p, dtype=float), 0, 1)\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    idx = np.digitize(p, bins) - 1\n",
    "    ece = 0.0\n",
    "    mce = 0.0\n",
    "    N = len(y)\n",
    "    for b in range(n_bins):\n",
    "        m = idx == b\n",
    "        if not np.any(m):\n",
    "            continue\n",
    "        acc = y[m].mean()      # observed rate in the bin\n",
    "        conf = p[m].mean()     # average predicted in the bin\n",
    "        gap = abs(acc - conf)\n",
    "        ece += gap * (m.sum() / N)\n",
    "        mce = max(mce, gap)\n",
    "    return float(ece), float(mce)\n",
    "\n",
    "def brier(y: np.ndarray, p: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Brier score = mean squared error between probability predictions and outcomes.\n",
    "    Range [0,1], smaller is better.\n",
    "    Why it’s good: a strictly proper scoring rule that rewards well-calibrated, sharp probabilities in one number.\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    p = np.clip(np.asarray(p, dtype=float), 0, 1)\n",
    "    return float(np.mean((p - y) ** 2))\n",
    "\n",
    "def log_loss_nll(y: np.ndarray, p: np.ndarray, eps: float = 1e-12) -> float:\n",
    "    \"\"\"\n",
    "    Negative log-likelihood (log loss) of probabilistic predictions.\n",
    "    Penalizes confident wrong predictions more. Smaller is better.\n",
    "    Why it’s good: a gold-standard proper scoring rule aligned with maximum likelihood; sensitive to overconfidence.\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    p = np.clip(np.asarray(p, dtype=float), eps, 1 - eps)\n",
    "    return float(-np.mean(y * np.log(p) + (1 - y) * np.log(1 - p)))\n",
    "\n",
    "def mean_pred_vs_obs(y: np.ndarray, p: np.ndarray) -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Compare average predicted risk to observed prevalence:\n",
    "    - mean_pred      : E[p]  (expected event rate across patients)\n",
    "    - observed_prev  : E[y]  (observed event rate / prevalence)\n",
    "    - E_over_O ratio : mean_pred / observed_prev  (≈1 is ideal)\n",
    "      >1 indicates global overestimation; <1 indicates underestimation.\n",
    "    Why it’s good: a simple, clinician-friendly “sanity check” for calibration-in-the-large before deeper analysis.\n",
    "    \"\"\"\n",
    "    p_mean = float(np.mean(p))\n",
    "    obs = float(np.mean(y))\n",
    "    e_over_o = (p_mean / obs) if obs > 0 else float(\"inf\")\n",
    "    return p_mean, obs, e_over_o\n",
    "\n",
    "def hosmer_lemeshow(y: np.ndarray, p: np.ndarray, g: int = 10) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Hosmer–Lemeshow goodness-of-fit test (grouped by predicted-risk deciles).\n",
    "    Returns {'HL_X2': chi-square, 'df': degrees_of_freedom}. Interpret cautiously (sample-size sensitive).\n",
    "    Why it’s good: a classic omnibus check that can flag severe miscalibration patterns; use alongside ECE and plots.\n",
    "    \"\"\"\n",
    "    order = np.argsort(p)\n",
    "    y_sorted = y[order]\n",
    "    p_sorted = p[order]\n",
    "    indices = np.array_split(np.arange(len(y)), g)\n",
    "    chi2 = 0.0\n",
    "    dof = g - 2\n",
    "    for idx in indices:\n",
    "        og = y_sorted[idx].sum()     # observed events in group\n",
    "        eg = p_sorted[idx].sum()     # expected events in group\n",
    "        ng = len(idx)\n",
    "        var = eg * (1 - (eg / max(ng, 1)))  # simple variance approx on counts\n",
    "        if var > 0:\n",
    "            chi2 += (og - eg) ** 2 / var\n",
    "    return {\"HL_X2\": float(chi2), \"df\": int(dof)}\n",
    "\n",
    "# ---------- Plot (optional visualization) ----------\n",
    "def plot_reliability(y: np.ndarray, p: np.ndarray, n_bins: int = 15, title: str = \"Reliability diagram\") -> None:\n",
    "    \"\"\"\n",
    "    Reliability diagram: observed event rate vs mean predicted probability per bin.\n",
    "    Perfect calibration lies on the diagonal (y = x).\n",
    "    Why it’s good: fast visual diagnosis of under/overestimation and nonlinear calibration issues.\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, dtype=int)\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    bins = np.linspace(0, 1, n_bins + 1)\n",
    "    idx = np.digitize(p, bins) - 1\n",
    "    xs, ys = [], []\n",
    "    for b in range(n_bins):\n",
    "        m = idx == b\n",
    "        if not np.any(m):\n",
    "            continue\n",
    "        xs.append(p[m].mean())\n",
    "        ys.append(y[m].mean())\n",
    "    xs, ys = np.array(xs), np.array(ys)\n",
    "    plt.figure()\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", label=\"Perfect\")\n",
    "    if len(xs) > 0:\n",
    "        plt.plot(xs, ys, marker=\"o\", label=\"Observed\")\n",
    "    plt.xlabel(\"Mean predicted probability (per bin)\")\n",
    "    plt.ylabel(\"Observed event rate (per bin)\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# ---------- DataFrame-based wrappers ----------\n",
    "def calibration_metrics_block(\n",
    "    df: pd.DataFrame,\n",
    "    prob_col: str = \"mortality_prob\",\n",
    "    y_col: str = \"died_within_30_days\",\n",
    "    n_bins: int = 15\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute a standard set of calibration metrics for a DataFrame block:\n",
    "      mean_pred, observed_prev, E_over_O, intercept/slope, ECE/MCE, Brier, NLL, HL (if big enough).\n",
    "    Returns a dict of named metrics.\n",
    "    Why it’s good: one call gives you a complete calibration snapshot that’s easy to table and compare.\n",
    "    \"\"\"\n",
    "    p = np.clip(df[prob_col].astype(float).values, 0, 1)\n",
    "    y = df[y_col].astype(int).values\n",
    "    a, b = calibration_slope_intercept(y, p)\n",
    "    ece, mce = ece_mce(y, p, n_bins=n_bins)\n",
    "    br = brier(y, p)\n",
    "    nll = log_loss_nll(y, p)\n",
    "    p_mean, obs, e_over_o = mean_pred_vs_obs(y, p)\n",
    "    hl = hosmer_lemeshow(y, p, g=min(10, max(5, int(np.sqrt(len(df)))))) if len(df) >= 1000 else {\"HL_X2\": np.nan, \"df\": np.nan}\n",
    "    return {\n",
    "        \"n\": int(len(df)),\n",
    "        \"events\": int(y.sum()),\n",
    "        \"mean_pred\": p_mean,\n",
    "        \"observed_prev\": obs,\n",
    "        \"E_over_O\": e_over_o,\n",
    "        \"cal_intercept\": a,   # target ~ 0\n",
    "        \"cal_slope\": b,       # target ~ 1\n",
    "        \"ECE\": ece,\n",
    "        \"MCE\": mce,\n",
    "        \"Brier\": br,\n",
    "        \"NLL\": nll,\n",
    "        \"HL_X2\": hl[\"HL_X2\"],\n",
    "        \"HL_df\": hl[\"df\"]\n",
    "    }\n",
    "\n",
    "def calibration_summary(\n",
    "    df: pd.DataFrame,\n",
    "    prob_col: str = \"mortality_prob\",\n",
    "    y_col: str = \"died_within_30_days\",\n",
    "    group_col: Optional[str] = None,\n",
    "    n_bins: int = 15\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute calibration metrics overall or grouped by `group_col`.\n",
    "    Returns a DataFrame indexed by group (or 'OVERALL').\n",
    "    Why it’s good: makes apples-to-apples calibration comparisons across gender/race/intersections trivial.\n",
    "    \"\"\"\n",
    "    if group_col is None:\n",
    "        m = calibration_metrics_block(df, prob_col, y_col, n_bins)\n",
    "        return pd.DataFrame([m], index=[\"OVERALL\"])\n",
    "    out = []\n",
    "    for g, sub in df.groupby(group_col):\n",
    "        m = calibration_metrics_block(sub, prob_col, y_col, n_bins)\n",
    "        m[\"group\"] = g\n",
    "        out.append(m)\n",
    "    return pd.DataFrame(out).set_index(\"group\").sort_index()\n",
    "\n",
    "# ---------- Recommendation logic ----------\n",
    "def recommend_calibration(\n",
    "    overall: Dict[str, float],\n",
    "    by_gender: Optional[pd.DataFrame] = None,\n",
    "    by_race: Optional[pd.DataFrame] = None,\n",
    "    min_events_per_group: int = 200\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Rule-based recommendation for calibration correction, based on intercept/slope/ECE and group consistency.\n",
    "    Returns a short human-readable recommendation string.\n",
    "    Why it’s good: gives you an actionable next step (e.g., logistic vs isotonic vs group-wise) without guesswork.\n",
    "    \"\"\"\n",
    "    msgs = []\n",
    "    a = overall.get(\"cal_intercept\", np.nan)\n",
    "    b = overall.get(\"cal_slope\", np.nan)\n",
    "    ece = overall.get(\"ECE\", np.nan)\n",
    "\n",
    "    intercept_bad = (not np.isnan(a)) and (abs(a) > 0.1)\n",
    "    slope_bad     = (not np.isnan(b)) and (abs(b - 1.0) > 0.1)\n",
    "    ece_bad       = (not np.isnan(ece)) and (ece > 0.03)\n",
    "\n",
    "    if not intercept_bad and not slope_bad and not ece_bad:\n",
    "        msgs.append(\"Overall calibration looks acceptable (intercept≈0, slope≈1, ECE small). No correction strictly needed.\")\n",
    "    else:\n",
    "        if intercept_bad and not slope_bad:\n",
    "            msgs.append(\"Calibration-in-the-large issue (intercept far from 0). → Intercept-only update or logistic recalibration.\")\n",
    "        if slope_bad:\n",
    "            if b < 1:\n",
    "                msgs.append(\"Slope < 1 (predictions too extreme). → Logistic recalibration or temperature scaling.\")\n",
    "            else:\n",
    "                msgs.append(\"Slope > 1 (predictions not extreme enough). → Logistic recalibration.\")\n",
    "        if ece_bad:\n",
    "            msgs.append(\"ECE is high (likely nonlinear miscalibration). → If enough data, isotonic; else beta calibration.\")\n",
    "\n",
    "    def _group_flag(df, label):\n",
    "        if df is None or len(df) == 0:\n",
    "            return\n",
    "        # Only consider groups with enough events (avoid noisy judgments)\n",
    "        df2 = df[df[\"events\"] >= min_events_per_group]\n",
    "        if df2.empty:\n",
    "            msgs.append(f\"{label}: many groups have few events; prefer simpler calibrators (logistic) or pool groups.\")\n",
    "            return\n",
    "        ia = df2[\"cal_intercept\"].astype(float)\n",
    "        ib = df2[\"cal_slope\"].astype(float)\n",
    "        ie = df2[\"ECE\"].astype(float)\n",
    "        if (ia.max() - ia.min()) > 0.2 or (ib.max() - ib.min()) > 0.3 or (ie.max() - ie.min()) > 0.03:\n",
    "            msgs.append(f\"{label}: notable between-group calibration differences. → Consider group-wise logistic recalibration (or multicalibration).\")\n",
    "        else:\n",
    "            msgs.append(f\"{label}: group calibration appears fairly consistent.\")\n",
    "\n",
    "    _group_flag(by_gender, \"Gender\")\n",
    "    _group_flag(by_race, \"Race\")\n",
    "\n",
    "    # Primary recommendation summary\n",
    "    if intercept_bad and not slope_bad and not ece_bad:\n",
    "        primary = \"Intercept-only update (recalibrate baseline risk).\"\n",
    "    elif slope_bad and not ece_bad:\n",
    "        primary = \"Logistic recalibration (Platt-style) or temperature scaling.\"\n",
    "    elif ece_bad:\n",
    "        primary = \"Isotonic regression (if data-rich) or Beta calibration (if moderate data).\"\n",
    "    else:\n",
    "        primary = \"No correction needed; monitor with temporal validation.\"\n",
    "\n",
    "    msgs.append(f\"Primary recommendation: {primary}\")\n",
    "    return \"\\n\".join(msgs)\n",
    "\n",
    "# ---------- One-call runner ----------\n",
    "def run_calibration_check(\n",
    "    df_clean: pd.DataFrame,\n",
    "    prob_col: str = \"mortality_prob\",\n",
    "    y_col: str = \"died_within_30_days\",\n",
    "    n_bins: int = 15,\n",
    "    do_plots: bool = True\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Run calibration measurement overall, by gender, by race, and by gender×race.\n",
    "    Prints tables + optional reliability plots, and returns a dict of DataFrames.\n",
    "    Why it’s good: a turnkey “calibration report” you can drop into your pipeline and cite in your thesis.\n",
    "    \"\"\"\n",
    "    # Build intersectional column if available\n",
    "    if \"gender\" in df_clean.columns and \"race_standard\" in df_clean.columns:\n",
    "        df_clean = df_clean.copy()\n",
    "        df_clean[\"gender_race\"] = df_clean[\"gender\"].astype(str) + \"|\" + df_clean[\"race_standard\"].astype(str)\n",
    "\n",
    "    # Compute summaries\n",
    "    overall = calibration_summary(df_clean, prob_col, y_col, group_col=None, n_bins=n_bins)\n",
    "    by_gender = calibration_summary(df_clean, prob_col, y_col, group_col=\"gender\", n_bins=n_bins) if \"gender\" in df_clean.columns else pd.DataFrame()\n",
    "    by_race = calibration_summary(df_clean, prob_col, y_col, group_col=\"race_standard\", n_bins=n_bins) if \"race_standard\" in df_clean.columns else pd.DataFrame()\n",
    "    by_ix = calibration_summary(df_clean, prob_col, y_col, group_col=\"gender_race\", n_bins=n_bins) if \"gender_race\" in df_clean.columns else pd.DataFrame()\n",
    "\n",
    "    # Optional reliability plots\n",
    "    if do_plots:\n",
    "        y = df_clean[y_col].astype(int).values\n",
    "        p = np.clip(df_clean[prob_col].astype(float).values, 0, 1)\n",
    "        plot_reliability(y, p, n_bins=n_bins, title=\"Reliability: OVERALL\")\n",
    "        if not by_gender.empty:\n",
    "            for g, sub in df_clean.groupby(\"gender\"):\n",
    "                plot_reliability(sub[y_col].astype(int).values, sub[prob_col].astype(float).values,\n",
    "                                 n_bins=n_bins, title=f\"Reliability: gender={g}\")\n",
    "        if not by_race.empty:\n",
    "            for r, sub in df_clean.groupby(\"race_standard\"):\n",
    "                plot_reliability(sub[y_col].astype(int).values, sub[prob_col].astype(float).values,\n",
    "                                 n_bins=n_bins, title=f\"Reliability: race={r}\")\n",
    "\n",
    "    # Recommendation text\n",
    "    rec_txt = recommend_calibration(overall.iloc[0].to_dict(),\n",
    "                                    by_gender if not by_gender.empty else None,\n",
    "                                    by_race if not by_race.empty else None)\n",
    "\n",
    "    # Print summaries\n",
    "    print(\"\\n=== CALIBRATION: OVERALL ===\")\n",
    "    print(overall.round(4).to_string())\n",
    "    if not by_gender.empty:\n",
    "        print(\"\\n=== CALIBRATION: BY GENDER ===\")\n",
    "        print(by_gender.round(4).to_string())\n",
    "    if not by_race.empty:\n",
    "        print(\"\\n=== CALIBRATION: BY RACE ===\")\n",
    "        print(by_race.round(4).to_string())\n",
    "    if not by_ix.empty:\n",
    "        print(\"\\n=== CALIBRATION: BY GENDER×RACE (intersection) ===\")\n",
    "        print(by_ix.round(4).to_string())\n",
    "\n",
    "    print(\"\\n=== RECOMMENDATION ===\")\n",
    "    print(rec_txt)\n",
    "\n",
    "    return {\n",
    "        \"overall\": overall,\n",
    "        \"by_gender\": by_gender,\n",
    "        \"by_race\": by_race,\n",
    "        \"by_gender_race\": by_ix,\n",
    "        \"recommendation\": rec_txt\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0ca627-1df4-4616-a04f-0712b8665814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 3: FAIRNESS/BIAS AUDIT HELPERS \n",
    "# =============================================================================\n",
    "from typing import Dict, Iterable, Any, Tuple\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, brier_score_loss\n",
    "\n",
    "def ensure_binary(y: pd.Series) -> np.ndarray:\n",
    "    yv = pd.to_numeric(y, errors=\"coerce\").astype(\"Int64\").fillna(0).astype(int).values\n",
    "    bad = set(np.unique(yv)) - {0, 1}\n",
    "    if bad:\n",
    "        raise ValueError(f\"died_within_30_days must be 0/1. Found {bad}.\")\n",
    "    return yv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ece_score(y_true: np.ndarray, y_prob: np.ndarray, n_bins: int = 15) -> float:\n",
    "    y_prob = np.clip(y_prob.astype(float), 0, 1)\n",
    "    bins = np.linspace(0, 1, n_bins + 1)\n",
    "    idx = np.digitize(y_prob, bins) - 1\n",
    "    ece = 0.0\n",
    "    n = len(y_true)\n",
    "    for b in range(n_bins):\n",
    "        m = idx == b\n",
    "        if not np.any(m):\n",
    "            continue\n",
    "        acc = y_true[m].mean()\n",
    "        conf = y_prob[m].mean()\n",
    "        ece += np.abs(acc - conf) * (m.sum() / n)\n",
    "    return float(ece)\n",
    "\n",
    "\n",
    "def calib_points(y_true: np.ndarray, y_prob: np.ndarray, n_bins: int = 15) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    y_prob = np.clip(y_prob.astype(float), 0, 1)\n",
    "    bins = np.linspace(0, 1, n_bins + 1)\n",
    "    idx = np.digitize(y_prob, bins) - 1\n",
    "    xs, ys = [], []\n",
    "    for b in range(n_bins):\n",
    "        m = idx == b\n",
    "        if not np.any(m): \n",
    "            continue\n",
    "        xs.append(y_prob[m].mean())\n",
    "        ys.append(y_true[m].mean())\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "\n",
    "\n",
    "def rates_at_threshold(y_true: np.ndarray, y_prob: np.ndarray, thr: float) -> Dict[str, float]:\n",
    "    y_pred = (y_prob >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "    tpr = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) else 0.0\n",
    "    tnr = 1 - fpr\n",
    "    fnr = 1 - tpr\n",
    "    ppv = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "    npv = tn / (tn + fn) if (tn + fn) else 0.0\n",
    "    prev = (tp + fn) / (tp + fp + tn + fn) if (tp + fp + tn + fn) else 0.0\n",
    "    return {\"TPR\": tpr, \"FPR\": fpr, \"TNR\": tnr, \"FNR\": fnr, \"PPV\": ppv, \"NPV\": npv, \"Prevalence\": prev,\n",
    "            \"TP\": tp, \"FP\": fp, \"TN\": tn, \"FN\": fn}\n",
    "\n",
    "\n",
    "# ---- “maximize X” rules --------------------------------------------------\n",
    "def youdens_j_threshold(y_true: np.ndarray, y_prob: np.ndarray) -> float:\n",
    "    fpr, tpr, thr = roc_curve(y_true, y_prob)\n",
    "    j = tpr - fpr\n",
    "    return float(thr[np.argmax(j)])\n",
    "\n",
    "\n",
    "\n",
    "def auc_by_group(df: pd.DataFrame, group_col: str) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for g, sub in df.groupby(group_col):\n",
    "        y = ensure_binary(sub[\"died_within_30_days\"])\n",
    "        p = sub[\"mortality_prob\"].astype(float).values\n",
    "        auc_val = np.nan if len(np.unique(y)) < 2 else roc_auc_score(y, p)\n",
    "        rows.append({\"group\": g, \"AUC\": auc_val, \"n\": len(sub)})\n",
    "    return pd.DataFrame(rows).set_index(\"group\")\n",
    "\n",
    "\n",
    "\n",
    "def brier_by_group(df: pd.DataFrame, group_col: str) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for g, sub in df.groupby(group_col):\n",
    "        y = ensure_binary(sub[\"died_within_30_days\"]).astype(float)\n",
    "        p = sub[\"mortality_prob\"].astype(float).values\n",
    "        rows.append({\"group\": g, \"Brier\": brier_score_loss(y, p), \"n\": len(sub)})\n",
    "    return pd.DataFrame(rows).set_index(\"group\")\n",
    "\n",
    "\n",
    "def ece_by_group(df: pd.DataFrame, group_col: str, n_bins: int = 15) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for g, sub in df.groupby(group_col):\n",
    "        y = ensure_binary(sub[\"died_within_30_days\"]).astype(int)\n",
    "        p = sub[\"mortality_prob\"].astype(float).values\n",
    "        rows.append({\"group\": g, \"ECE\": ece_score(y, p, n_bins=n_bins), \"n\": len(sub)})\n",
    "    return pd.DataFrame(rows).set_index(\"group\")\n",
    "\n",
    "\n",
    "\n",
    "def base_rate_by_group(df: pd.DataFrame, group_col: str) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for g, sub in df.groupby(group_col):\n",
    "        rows.append({\"group\": g, \"Prevalence\": float(sub[\"died_within_30_days\"].mean()), \"n\": len(sub)})\n",
    "    return pd.DataFrame(rows).set_index(\"group\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d04e9c5-7956-4985-abed-29b6ddcd7bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other threshold functions \n",
    "    '''\n",
    "    (quick guide)\n",
    "    - Clinical sensitivity target (avoid missed deaths): threshold_target_tpr (e.g., TPR ≥ 0.90).\n",
    "    - Resource cap (e.g., only top 10% get extra tests): threshold_top_k.\n",
    "    - Balanced errors: threshold_youden_j or threshold_max_f1.\n",
    "    - Cost asymmetry (FN >> FP): bayes_threshold_from_costs (if well-calibrated) or threshold_min_expected_cost (agnostic).\n",
    "    - Fairness—global policy: threshold_min_eo_gap or threshold_min_eqopp_gap.\n",
    "    - Fairness—group-specific policy: group_thresholds_for_target_tpr / …_fpr.\n",
    "    '''\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3B: THRESHOLD SELECTION HELPERS (copy/paste this block)_- Thresholdsssssssssssssssssssssssssssssssssssss\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple, Iterable, Any\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score\n",
    "\n",
    "# ---- Basic utilities --------------------------------------------------------\n",
    "def _ensure_binary(y: pd.Series) -> np.ndarray:\n",
    "    yv = pd.to_numeric(y, errors=\"coerce\").fillna(0).astype(int).values\n",
    "    if not set(np.unique(yv)).issubset({0,1}):\n",
    "        raise ValueError(\"died_within_30_days must be 0/1.\")\n",
    "    return yv\n",
    "\n",
    "def _confusion(y_true: np.ndarray, y_prob: np.ndarray, thr: float) -> Tuple[int,int,int,int]:\n",
    "    y_pred = (y_prob >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "    return tp, fp, tn, fn\n",
    "\n",
    "def _metrics(tp:int, fp:int, tn:int, fn:int) -> Dict[str, float]:\n",
    "    tpr = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) else 0.0\n",
    "    tnr = 1 - fpr\n",
    "    fnr = 1 - tpr\n",
    "    ppv = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "    npv = tn / (tn + fn) if (tn + fn) else 0.0\n",
    "    prec, rec = ppv, tpr\n",
    "    f1 = 2*prec*rec/(prec+rec) if (prec+rec) else 0.0\n",
    "    return {\"TPR\": tpr, \"FPR\": fpr, \"TNR\": tnr, \"FNR\": fnr, \"PPV\": ppv, \"NPV\": npv, \"F1\": f1}\n",
    "\n",
    "def _candidate_thresholds(y_prob: np.ndarray, n: int = 512) -> np.ndarray:\n",
    "    lo, hi = float(np.min(y_prob)), float(np.max(y_prob))\n",
    "    uniq = np.unique(y_prob)\n",
    "    if len(uniq) <= n:\n",
    "        # extend ends slightly to include all decision flips\n",
    "        return np.concatenate(([lo-1e-12], uniq, [hi+1e-12]))\n",
    "    return np.linspace(lo, hi, n)\n",
    "\n",
    "def scan_thresholds(y_true: np.ndarray, y_prob: np.ndarray, candidates: np.ndarray = None) -> pd.DataFrame:\n",
    "    if candidates is None:\n",
    "        candidates = _candidate_thresholds(y_prob)\n",
    "    rows = []\n",
    "    for thr in candidates:\n",
    "        tp, fp, tn, fn = _confusion(y_true, y_prob, thr)\n",
    "        m = _metrics(tp, fp, tn, fn)\n",
    "        m.update({\"threshold\": float(thr), \"TP\": tp, \"FP\": fp, \"TN\": tn, \"FN\": fn})\n",
    "        rows.append(m)\n",
    "    return pd.DataFrame(rows).sort_values(\"threshold\").reset_index(drop=True)\n",
    "\n",
    "# ---- “maximize X” rules -----------------------------------------------------\n",
    "def threshold_youden_j(y_true: np.ndarray, y_prob: np.ndarray) -> float:\n",
    "    fpr, tpr, thr = roc_curve(y_true, y_prob)\n",
    "    j = tpr - fpr\n",
    "    return float(thr[np.argmax(j)])\n",
    "\n",
    "def threshold_max_f1(y_true: np.ndarray, y_prob: np.ndarray) -> float:\n",
    "    tbl = scan_thresholds(y_true, y_prob)\n",
    "    i = int(tbl[\"F1\"].values.argmax())\n",
    "    return float(tbl.loc[i, \"threshold\"])\n",
    "\n",
    "def threshold_max_fbeta(y_true: np.ndarray, y_prob: np.ndarray, beta: float = 2.0) -> float:\n",
    "    tbl = scan_thresholds(y_true, y_prob)\n",
    "    prec, rec = tbl[\"PPV\"].values, tbl[\"TPR\"].values\n",
    "    fbeta = (1+beta**2) * (prec*rec) / (beta**2*prec + rec + 1e-12)\n",
    "    return float(tbl.loc[int(fbeta.argmax()), \"threshold\"])\n",
    "\n",
    "# ---- Resource / policy constraints -----------------------------------------\n",
    "def threshold_top_k(y_prob: np.ndarray, k: float) -> float:\n",
    "    \"\"\"\n",
    "    Select top k fraction as positive. k=0.10 => label top 10% highest risk as positive.\n",
    "    Returns the score cutoff (inclusive).\n",
    "    \"\"\"\n",
    "    assert 0 < k < 1, \"k should be in (0,1)\"\n",
    "    return float(np.quantile(y_prob, 1 - k))\n",
    "\n",
    "def threshold_target_tpr(y_true: np.ndarray, y_prob: np.ndarray, target_tpr: float) -> float:\n",
    "    \"\"\"\n",
    "    Smallest threshold that achieves TPR >= target_tpr (maximizes specificity subject to sensitivity).\n",
    "    \"\"\"\n",
    "    fpr, tpr, thr = roc_curve(y_true, y_prob)\n",
    "    ok = np.where(tpr >= target_tpr)[0]\n",
    "    if len(ok) == 0:\n",
    "        return float(thr[np.argmax(tpr)])  # highest achievable TPR\n",
    "    return float(thr[ok[0]])\n",
    "\n",
    "def threshold_target_fpr(y_true: np.ndarray, y_prob: np.ndarray, target_fpr: float) -> float:\n",
    "    \"\"\"\n",
    "    Largest threshold with FPR <= target_fpr (maximizes TPR subject to specificity).\n",
    "    \"\"\"\n",
    "    fpr, tpr, thr = roc_curve(y_true, y_prob)\n",
    "    ok = np.where(fpr <= target_fpr)[0]\n",
    "    if len(ok) == 0:\n",
    "        return float(thr[np.argmin(fpr)])  # lowest achievable FPR\n",
    "    # among ok, pick the one with highest TPR\n",
    "    best = ok[np.argmax(tpr[ok])]\n",
    "    return float(thr[best])\n",
    "\n",
    "def threshold_target_ppv(y_true: np.ndarray, y_prob: np.ndarray, target_ppv: float) -> float:\n",
    "    \"\"\"\n",
    "    Smallest threshold that yields PPV >= target_ppv.\n",
    "    Note: PPV is not monotone in threshold, so we scan.\n",
    "    \"\"\"\n",
    "    tbl = scan_thresholds(y_true, y_prob)\n",
    "    ok = tbl.index[tbl[\"PPV\"] >= target_ppv].tolist()\n",
    "    return float(tbl.loc[ok[0], \"threshold\"]) if ok else float(tbl.loc[tbl[\"PPV\"].idxmax(), \"threshold\"])\n",
    "\n",
    "# ---- Cost/utility based -----------------------------------------------------\n",
    "#      Bayes (cost-based) (calibrated)\n",
    "def bayes_threshold_from_costs(prevalence: float, cost_fn: float = 5.0, cost_fp: float = 1.0) -> float:\n",
    "    \"\"\"\n",
    "    Bayes decision rule assuming calibrated probabilities:\n",
    "    predict 1 if p >= τ*, where τ* = C_FP*(1-π) / [C_FN*π + C_FP*(1-π)]\n",
    "    \"\"\"\n",
    "    pi = float(prevalence)\n",
    "    return float((cost_fp*(1 - pi)) / (cost_fn*pi + cost_fp*(1 - pi)))\n",
    "\n",
    "def threshold_min_expected_cost(y_true: np.ndarray, y_prob: np.ndarray, cost_fn: float = 5.0, cost_fp: float = 1.0) -> float:\n",
    "    \"\"\"\n",
    "    Choose threshold that minimizes Expected Cost = cost_fn*FN + cost_fp*FP\n",
    "    (does NOT assume perfect calibration; uses empirical counts).\n",
    "    \"\"\"\n",
    "    tbl = scan_thresholds(y_true, y_prob)\n",
    "    exp_cost = cost_fn*tbl[\"FN\"].values + cost_fp*tbl[\"FP\"].values\n",
    "    return float(tbl.loc[int(exp_cost.argmin()), \"threshold\"])\n",
    "\n",
    "# ---- Fairness-aware search (global thresholds) ------------------------------\n",
    "def _rates_by_group(y_true: np.ndarray, y_prob: np.ndarray, groups: Iterable[Any], thr: float) -> pd.DataFrame:\n",
    "    df = pd.DataFrame({\"y\": y_true, \"p\": y_prob, \"g\": np.array(list(groups))})\n",
    "    rows = []\n",
    "    for g, sub in df.groupby(\"g\"):\n",
    "        tp, fp, tn, fn = _confusion(sub[\"y\"].values, sub[\"p\"].values, thr)\n",
    "        m = _metrics(tp, fp, tn, fn); m[\"group\"] = g\n",
    "        rows.append(m)\n",
    "    return pd.DataFrame(rows).set_index(\"group\")\n",
    "\n",
    "def threshold_min_eo_gap(y_true: np.ndarray, y_prob: np.ndarray, groups: Iterable[Any], n_candidates: int = 200) -> Tuple[float, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Minimize Equalized Odds gaps: argmin_thr [ (max TPR - min TPR) + (max FPR - min FPR) ].\n",
    "    \"\"\"\n",
    "    lo, hi = float(np.min(y_prob)), float(np.max(y_prob))\n",
    "    candidates = np.linspace(lo, hi, n_candidates)\n",
    "    best = (None, None, np.inf)\n",
    "    for thr in candidates:\n",
    "        per = _rates_by_group(y_true, y_prob, groups, thr)\n",
    "        gap = (per[\"TPR\"].max() - per[\"TPR\"].min()) + (per[\"FPR\"].max() - per[\"FPR\"].min())\n",
    "        if float(gap) < best[2]:\n",
    "            best = (float(thr), per, float(gap))\n",
    "    return best[0], best[1]\n",
    "\n",
    "def threshold_min_eqopp_gap(y_true: np.ndarray, y_prob: np.ndarray, groups: Iterable[Any], n_candidates: int = 200) -> Tuple[float, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Minimize Equality of Opportunity gap: argmin_thr [ max TPR - min TPR ].\n",
    "    \"\"\"\n",
    "    lo, hi = float(np.min(y_prob)), float(np.max(y_prob))\n",
    "    candidates = np.linspace(lo, hi, n_candidates)\n",
    "    best = (None, None, np.inf)\n",
    "    for thr in candidates:\n",
    "        per = _rates_by_group(y_true, y_prob, groups, thr)\n",
    "        gap = per[\"TPR\"].max() - per[\"TPR\"].min()\n",
    "        if float(gap) < best[2]:\n",
    "            best = (float(thr), per, float(gap))\n",
    "    return best[0], best[1]\n",
    "\n",
    "# ---- Group-specific thresholds (policy choice) ------------------------------\n",
    "def group_thresholds_for_target_tpr(y_true: np.ndarray, y_prob: np.ndarray, groups: Iterable[Any], target_tpr: float) -> Dict[Any, float]:\n",
    "    \"\"\"\n",
    "    For each group, pick the smallest threshold that achieves TPR >= target_tpr (Eq. Opportunity via group-specific cutoffs).\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\"y\": y_true, \"p\": y_prob, \"g\": np.array(list(groups))})\n",
    "    out = {}\n",
    "    for g, sub in df.groupby(\"g\"):\n",
    "        out[g] = threshold_target_tpr(sub[\"y\"].values, sub[\"p\"].values, target_tpr)\n",
    "    return out\n",
    "\n",
    "def group_thresholds_for_target_fpr(y_true: np.ndarray, y_prob: np.ndarray, groups: Iterable[Any], target_fpr: float) -> Dict[Any, float]:\n",
    "    \"\"\"\n",
    "    For each group, pick the largest threshold with FPR <= target_fpr (specificity constraint).\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\"y\": y_true, \"p\": y_prob, \"g\": np.array(list(groups))})\n",
    "    out = {}\n",
    "    for g, sub in df.groupby(\"g\"):\n",
    "        out[g] = threshold_target_fpr(sub[\"y\"].values, sub[\"p\"].values, target_fpr)\n",
    "    return out\n",
    "\n",
    "# =============================================================================\n",
    "# EXAMPLE USAGE (uncomment and run after Step 2)\n",
    "# =============================================================================\n",
    "# y = _ensure_binary(df_clean[\"died_within_30_days\"])\n",
    "# p = df_clean[\"mortality_prob\"].astype(float).values\n",
    "# g_gender = df_clean[\"gender\"].astype(str).values\n",
    "#\n",
    "# thr_j     = threshold_youden_j(y, p)\n",
    "# thr_f1    = threshold_max_f1(y, p)\n",
    "# thr_fb    = threshold_max_fbeta(y, p, beta=2.0)\n",
    "# thr_tpr90 = threshold_target_tpr(y, p, target_tpr=0.90)\n",
    "# thr_fpr05 = threshold_target_fpr(y, p, target_fpr=0.05)\n",
    "# thr_top10 = threshold_top_k(p, k=0.10)\n",
    "# thr_ppv50 = threshold_target_ppv(y, p, target_ppv=0.50)\n",
    "#\n",
    "# # Cost-based (assuming misclassifying a death (FN) is 5x worse than a FP):\n",
    "# pi = float(y.mean())\n",
    "# thr_bayes = bayes_threshold_from_costs(prevalence=pi, cost_fn=5.0, cost_fp=1.0)\n",
    "# thr_min_cost = threshold_min_expected_cost(y, p, cost_fn=5.0, cost_fp=1.0)\n",
    "#\n",
    "# # Fairness-aware (global threshold):\n",
    "# thr_eo, per_eo = threshold_min_eo_gap(y, p, g_gender)\n",
    "# thr_eqopp, per_eqopp = threshold_min_eqopp_gap(y, p, g_gender)\n",
    "#\n",
    "# # Group-specific thresholds (policy choice):\n",
    "# thr_by_gender_eqopp = group_thresholds_for_target_tpr(y, p, g_gender, target_tpr=0.90)\n",
    "# thr_by_gender_spec  = group_thresholds_for_target_fpr(y, p, g_gender, target_fpr=0.05)\n",
    "#\n",
    "# print(\"YoudenJ:\", thr_j)\n",
    "# print(\"Max F1:\", thr_f1)\n",
    "# print(\"Max Fβ (β=2):\", thr_fb)\n",
    "# print(\"Target TPR 0.90:\", thr_tpr90)\n",
    "# print(\"Target FPR 0.05:\", thr_fpr05)\n",
    "# print(\"Top-10%:\", thr_top10)\n",
    "# print(\"Target PPV 0.50:\", thr_ppv50)\n",
    "# print(\"Bayes (cost_fn=5,cost_fp=1, π=%.3f):\" % pi, thr_bayes)\n",
    "# print(\"Min expected cost:\", thr_min_cost)\n",
    "# print(\"Min EO gap (global):\", thr_eo)\n",
    "# print(\"Min EqOpp gap (global):\", thr_eqopp)\n",
    "# print(\"Per-group EqOpp thresholds:\", thr_by_gender_eqopp)\n",
    "# print(\"Per-group specificity thresholds:\", thr_by_gender_spec)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f011d76-5181-4739-a52d-dfcb2f9c47f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrations category II >>  Modifying predictions to improve fairness/ calibration_optional \n",
    "# If you want to intervention but not necessary for bias detection\n",
    "\n",
    "# Logistic recalibration on a validation set\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def fit_logistic_recal(val_df, prob_col=\"mortality_prob\", y_col=\"died_within_30_days\"):\n",
    "    p = np.clip(val_df[prob_col].astype(float).values, 1e-6, 1-1e-6)\n",
    "    z = np.log(p/(1-p))  # logit\n",
    "    X = z.reshape(-1,1)\n",
    "    y = val_df[y_col].astype(int).values\n",
    "    lr = LogisticRegression(solver=\"lbfgs\")\n",
    "    lr.fit(X, y)\n",
    "    return lr  # use predict_proba on new logits\n",
    "\n",
    "def apply_logistic_recal(lr, test_df, prob_col=\"mortality_prob\"):\n",
    "    p = np.clip(test_df[prob_col].astype(float).values, 1e-6, 1-1e-6)\n",
    "    z = np.log(p/(1-p)).reshape(-1,1)\n",
    "    return lr.predict_proba(z)[:,1]\n",
    "\n",
    "# Isotonic regression (needs enough data)\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "def fit_isotonic(val_df, prob_col=\"mortality_prob\", y_col=\"died_within_30_days\"):\n",
    "    iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "    p = val_df[prob_col].astype(float).values\n",
    "    y = val_df[y_col].astype(int).values\n",
    "    iso.fit(p, y)\n",
    "    return iso\n",
    "\n",
    "def apply_isotonic(iso, test_df, prob_col=\"mortality_prob\"):\n",
    "    return np.clip(iso.predict(test_df[prob_col].astype(float).values), 0, 1)\n",
    "\n",
    "# Beta calibration (parametric, flexible) - simple logistic on log(p) & log(1-p)\n",
    "def fit_beta_cal(val_df, prob_col=\"mortality_prob\", y_col=\"died_within_30_days\"):\n",
    "    p = np.clip(val_df[prob_col].astype(float).values, 1e-6, 1-1e-6)\n",
    "    X = np.column_stack([np.log(p), np.log(1-p)])\n",
    "    y = val_df[y_col].astype(int).values\n",
    "    lr = LogisticRegression(solver=\"lbfgs\")\n",
    "    lr.fit(X, y)\n",
    "    return lr\n",
    "\n",
    "def apply_beta_cal(lr, test_df, prob_col=\"mortality_prob\"):\n",
    "    p = np.clip(test_df[prob_col].astype(float).values, 1e-6, 1-1e-6)\n",
    "    X = np.column_stack([np.log(p), np.log(1-p)])\n",
    "    return lr.predict_proba(X)[:,1]\n",
    "\n",
    "# Group-wise logistic recalibration (allow different intercept/slope by group)\n",
    "def fit_group_logistic_recal(val_df, group_col, prob_col=\"mortality_prob\", y_col=\"died_within_30_days\"):\n",
    "    df = val_df.copy()\n",
    "    p = np.clip(df[prob_col].astype(float).values, 1e-6, 1-1e-6)\n",
    "    df[\"logit_p\"] = np.log(p/(1-p))\n",
    "    # one-hot group + interaction with logit_p\n",
    "    X = pd.get_dummies(df[group_col], drop_first=True)\n",
    "    X = pd.concat([df[[\"logit_p\"]], X, X.mul(df[\"logit_p\"], axis=0)], axis=1)\n",
    "    y = df[y_col].astype(int).values\n",
    "    lr = LogisticRegression(max_iter=1000)\n",
    "    lr.fit(X.values, y)\n",
    "    return lr, X.columns\n",
    "\n",
    "def apply_group_logistic_recal(lr, cols, test_df, group_col, prob_col=\"mortality_prob\"):\n",
    "    df = test_df.copy()\n",
    "    p = np.clip(df[prob_col].astype(float).values, 1e-6, 1-1e-6)\n",
    "    df[\"logit_p\"] = np.log(p/(1-p))\n",
    "    X = pd.get_dummies(df[group_col], drop_first=True)\n",
    "    X = pd.concat([df[[\"logit_p\"]], X, X.mul(df[\"logit_p\"], axis=0)], axis=1)\n",
    "    X = X.reindex(columns=cols, fill_value=0.0)\n",
    "    return lr.predict_proba(X.values)[:,1]\n",
    "i\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f74a4d1-895b-4e2b-a975-6a61d46c982a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6374a54-3fda-45fb-b1ff-ba92176140ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main for running \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
